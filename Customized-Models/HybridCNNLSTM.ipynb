{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d44cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "print(\"CUDA_VISIBLE_DEVICES =\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "!pip install python-docx seaborn pillow --quiet\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import copy\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(\"Running on device:\", device)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DATA_ROOT  = '/content/drive/MyDrive/dp/data'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/dp/output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE    = 32\n",
    "NUM_EPOCHS    = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "BEST_MODEL_PTH = os.path.join(OUTPUT_DIR, 'best_model.pth')\n",
    "BEST_ACC_TXT   = os.path.join(OUTPUT_DIR, 'best_val_acc.txt')\n",
    "CLF_TXT        = os.path.join(OUTPUT_DIR, 'classification_report.txt')\n",
    "CM_PNG         = os.path.join(OUTPUT_DIR, 'confusion_matrix.png')\n",
    "\n",
    "class Transform:\n",
    "    def __init__(self):\n",
    "        self.mean = np.array([0.485]*3, dtype=np.float32)\n",
    "        self.std  = np.array([0.229]*3, dtype=np.float32)\n",
    "    def __call__(self, img: Image.Image):\n",
    "        img = img.resize((224,224))\n",
    "        arr = np.array(img.convert('RGB'), dtype=np.float32)/255.0\n",
    "        arr = (arr - self.mean)/self.std\n",
    "        return torch.from_numpy(arr).permute(2,0,1)\n",
    "\n",
    "transform = Transform()\n",
    "\n",
    "class AnnotDataset(Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        self.samples = []\n",
    "        self.classes = sorted(d for d in os.listdir(root) if os.path.isdir(os.path.join(root,d)))\n",
    "        self.cl2i    = {c:i for i,c in enumerate(self.classes)}\n",
    "        for c in self.classes:\n",
    "            for fn in os.listdir(os.path.join(root,c)):\n",
    "                if fn.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "                    self.samples.append((os.path.join(root,c,fn), self.cl2i[c]))\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        p,l = self.samples[idx]\n",
    "        img = Image.open(p)\n",
    "        return self.transform(img), l\n",
    "\n",
    "full_ds = AnnotDataset(DATA_ROOT, transform)\n",
    "NUM_CLASSES = len(full_ds.classes)\n",
    "n = len(full_ds)\n",
    "n_train = int(0.8*n)\n",
    "n_val   = n - n_train\n",
    "train_ds, val_ds = random_split(full_ds, [n_train, n_val])\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_ld   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Conv2d(3,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(256,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((7,7))\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.f(x)\n",
    "\n",
    "class HybridCNNLSTM(nn.Module):\n",
    "    def __init__(self, num_classes, hid=256, layers=2, dp=0.5):\n",
    "        super().__init__()\n",
    "        self.cnn = SimpleCNN()\n",
    "        self.lstm = nn.LSTM(512, hid, num_layers=layers,\n",
    "                            batch_first=True, bidirectional=True, dropout=dp)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hid*2,128), nn.ReLU(), nn.Dropout(dp),\n",
    "            nn.Linear(128,num_classes)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        f = self.cnn(x)\n",
    "        B,C,H,W = f.shape\n",
    "        seq = f.view(B,C,H*W).permute(0,2,1)\n",
    "        _,(h,_) = self.lstm(seq)\n",
    "        h = torch.cat([h[-2],h[-1]], dim=1)\n",
    "        return self.head(h)\n",
    "\n",
    "model = HybridCNNLSTM(NUM_CLASSES).cpu()\n",
    "crit  = nn.CrossEntropyLoss()\n",
    "opt   = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val=0.0\n",
    "best_wts=copy.deepcopy(model.state_dict())\n",
    "hist={'tl':[],'ta':[],'vl':[],'va':[]}\n",
    "\n",
    "for e in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    t_loss=t_corr=t_cnt=0\n",
    "    for x,y in train_ld:\n",
    "        logits = model(x)\n",
    "        loss   = crit(logits, y)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        t_loss+= loss.item()*x.size(0)\n",
    "        preds  = logits.argmax(1)\n",
    "        t_corr += (preds==y).sum().item()\n",
    "        t_cnt  += y.size(0)\n",
    "    tl,ta = t_loss/t_cnt, t_corr/t_cnt\n",
    "\n",
    "    model.eval()\n",
    "    v_loss=v_corr=v_cnt=0\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_ld:\n",
    "            lo = model(x)\n",
    "            l  = crit(lo,y)\n",
    "            v_loss+= l.item()*x.size(0)\n",
    "            pr = lo.argmax(1)\n",
    "            v_corr += (pr==y).sum().item()\n",
    "            v_cnt  += y.size(0)\n",
    "    vl,va = v_loss/v_cnt, v_corr/v_cnt\n",
    "\n",
    "    hist['tl'].append(tl); hist['ta'].append(ta)\n",
    "    hist['vl'].append(vl); hist['va'].append(va)\n",
    "\n",
    "    if va>best_val:\n",
    "        best_val=va\n",
    "        best_wts=copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_wts, BEST_MODEL_PTH)\n",
    "\n",
    "    print(f\"Epoch {e}/{NUM_EPOCHS} â†’ Train: {tl:.4f}/{ta:.4f}  Val: {vl:.4f}/{va:.4f}\")\n",
    "\n",
    "with open(BEST_ACC_TXT,'w') as f: f.write(f\"{best_val:.4f}\\n\")\n",
    "\n",
    "plt.figure(); plt.plot(hist['tl'], label='Train Loss'); plt.plot(hist['vl'], label='Val Loss')\n",
    "plt.legend(); plt.savefig(os.path.join(OUTPUT_DIR,'loss_curve.png')); plt.close()\n",
    "plt.figure(); plt.plot(hist['ta'], label='Train Acc'); plt.plot(hist['va'], label='Val Acc')\n",
    "plt.legend(); plt.savefig(os.path.join(OUTPUT_DIR,'acc_curve.png')); plt.close()\n",
    "\n",
    "model.load_state_dict(best_wts)\n",
    "all_y, all_p = [],[]\n",
    "with torch.no_grad():\n",
    "    for x,y in val_ld:\n",
    "        preds = model(x).argmax(1).numpy()\n",
    "        all_p.extend(preds); all_y.extend(y.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_y, all_p)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=full_ds.classes, yticklabels=full_ds.classes)\n",
    "plt.savefig(CM_PNG); plt.close()\n",
    "\n",
    "rep = classification_report(all_y, all_p, target_names=full_ds.classes)\n",
    "with open(CLF_TXT,'w') as f: f.write(rep)\n",
    "\n",
    "doc=Document()\n",
    "doc.add_heading('Pneumonia Detection Report', level=1)\n",
    "doc.add_paragraph(f\"Best Val Acc: {best_val:.4f}\")\n",
    "doc.add_heading('Confusion Matrix', level=2); doc.add_picture(CM_PNG, width=Inches(5))\n",
    "doc.add_heading('Classification Report', level=2); doc.add_paragraph(rep)\n",
    "doc.add_heading('Training/Validation Curves', level=2)\n",
    "doc.add_picture(os.path.join(OUTPUT_DIR,'loss_curve.png'), width=Inches(5))\n",
    "doc.add_picture(os.path.join(OUTPUT_DIR,'acc_curve.png'), width=Inches(5))\n",
    "doc.save(os.path.join(OUTPUT_DIR,'report.docx'))\n",
    "\n",
    "print(\"Done! All outputs in\", OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
