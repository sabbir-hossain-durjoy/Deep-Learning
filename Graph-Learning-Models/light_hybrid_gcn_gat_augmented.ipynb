{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263df965",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torch-geometric scikit-learn tqdm seaborn matplotlib\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from itertools import cycle\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "input_folder = \"/content/drive/MyDrive/defense/data/test\"\n",
    "output_path  = \"/content/drive/MyDrive/defense/output\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=input_folder)\n",
    "class_names = full_dataset.classes\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "idx = np.arange(len(full_dataset.targets))\n",
    "idx_train, idx_test = train_test_split(idx, test_size=0.3, stratify=full_dataset.targets, random_state=42)\n",
    "idx_val, idx_test  = train_test_split(idx_test, test_size=0.5, stratify=np.array(full_dataset.targets)[idx_test], random_state=42)\n",
    "\n",
    "train_dataset = Subset(datasets.ImageFolder(root=input_folder, transform=train_transform), idx_train)\n",
    "val_dataset   = Subset(datasets.ImageFolder(root=input_folder, transform=test_transform), idx_val)\n",
    "test_dataset  = Subset(datasets.ImageFolder(root=input_folder, transform=test_transform), idx_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "feature_extractor = models.resnet18(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])\n",
    "feature_extractor.eval().to(device)\n",
    "\n",
    "def extract_features(loader):\n",
    "    feats, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader, desc=\"Extracting Features\"):\n",
    "            images = images.to(device)\n",
    "            outputs = feature_extractor(images)\n",
    "            outputs = outputs.squeeze(-1).squeeze(-1)\n",
    "            feats.append(outputs.cpu())\n",
    "            labs.append(targets.cpu())\n",
    "    return torch.cat(feats).numpy(), torch.cat(labs).numpy()\n",
    "\n",
    "features_train, labels_train = extract_features(train_loader)\n",
    "features_val,   labels_val   = extract_features(val_loader)\n",
    "features_test,  labels_test  = extract_features(test_loader)\n",
    "\n",
    "features = np.concatenate([features_train, features_val, features_test], axis=0)\n",
    "labels   = np.concatenate([labels_train, labels_val, labels_test], axis=0)\n",
    "\n",
    "idx_train = np.arange(len(labels_train))\n",
    "idx_val   = np.arange(len(labels_train), len(labels_train)+len(labels_val))\n",
    "idx_test  = np.arange(len(labels_train)+len(labels_val), len(labels))\n",
    "\n",
    "adj = kneighbors_graph(features, n_neighbors=5, metric=\"cosine\", mode=\"connectivity\", include_self=False)\n",
    "edge_index = torch.tensor(np.array(adj.nonzero()), dtype=torch.long)\n",
    "\n",
    "data = Data(\n",
    "    x=torch.tensor(features, dtype=torch.float),\n",
    "    edge_index=edge_index,\n",
    "    y=torch.tensor(labels, dtype=torch.long)\n",
    ").to(device)\n",
    "\n",
    "class LightHybridGCNGAT(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=2):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.gat1 = GATConv(hidden_dim, hidden_dim, heads=heads, dropout=0.5)\n",
    "        self.fc1  = nn.Linear(hidden_dim * heads, hidden_dim)\n",
    "        self.fc2  = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim * heads)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x_gcn = F.relu(self.gcn1(x, edge_index))\n",
    "        x_gat = F.elu(self.gat1(x_gcn, edge_index))\n",
    "        x = self.bn(x_gat)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def train_model(model, name, max_epochs=10000):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.003, weight_decay=1e-4)\n",
    "    criterion = nn.NLLLoss()\n",
    "    best_val_acc, best_test_acc, best_epoch = 0, 0, 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"test_loss\": [],\n",
    "               \"train_acc\": [], \"val_acc\": [], \"test_acc\": []}\n",
    "    out = None\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[idx_train], data.y[idx_train])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            losses = [\n",
    "                criterion(out[idx_train], data.y[idx_train]).item(),\n",
    "                criterion(out[idx_val],   data.y[idx_val]).item(),\n",
    "                criterion(out[idx_test],  data.y[idx_test]).item()\n",
    "            ]\n",
    "            accs = [\n",
    "                (pred[idx_train] == data.y[idx_train]).sum().item()/len(idx_train),\n",
    "                (pred[idx_val]   == data.y[idx_val]).sum().item()/len(idx_val),\n",
    "                (pred[idx_test]  == data.y[idx_test]).sum().item()/len(idx_test)\n",
    "            ]\n",
    "\n",
    "        history[\"train_loss\"].append(losses[0])\n",
    "        history[\"val_loss\"].append(losses[1])\n",
    "        history[\"test_loss\"].append(losses[2])\n",
    "        history[\"train_acc\"].append(accs[0])\n",
    "        history[\"val_acc\"].append(accs[1])\n",
    "        history[\"test_acc\"].append(accs[2])\n",
    "\n",
    "        if accs[1] > best_val_acc:\n",
    "            best_val_acc, best_test_acc, best_epoch = accs[1], accs[2], epoch\n",
    "            torch.save(model.state_dict(), os.path.join(output_path, f\"{name}_best.pth\"))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Train {accs[0]:.3f}/{losses[0]:.3f} | \"\n",
    "                  f\"Val {accs[1]:.3f}/{losses[1]:.3f} | Test {accs[2]:.3f}/{losses[2]:.3f}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(os.path.join(output_path, f\"{name}_best.pth\")))\n",
    "    return best_val_acc, best_test_acc, best_epoch, history, out\n",
    "\n",
    "hybrid_model = LightHybridGCNGAT(data.num_features, 32, len(class_names), heads=2)\n",
    "best_val, best_test, epoch, history, out = train_model(hybrid_model, \"LightHybrid\")\n",
    "\n",
    "print(\"Best Epoch:\", epoch)\n",
    "print(\"Validation Accuracy:\", best_val)\n",
    "print(\"Test Accuracy:\", best_test)\n",
    "\n",
    "epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.plot(epochs, history[\"test_loss\"], label=\"Test Loss\")\n",
    "plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\"); plt.title(\"Loss Curve\"); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
    "plt.plot(epochs, history[\"test_acc\"], label=\"Test Acc\")\n",
    "plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy Curve\"); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "pred = out.argmax(dim=1)\n",
    "cm = confusion_matrix(data.y[idx_test].cpu(), pred[idx_test].cpu())\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names, cmap=\"mako\")\n",
    "plt.title(\"LightHybrid GCN+GAT Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "y_test = data.y[idx_test].cpu().numpy()\n",
    "probs  = F.softmax(out[idx_test], dim=1).cpu().numpy()\n",
    "\n",
    "if probs.shape[1] == 2:\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Positive Class)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.arange(probs.shape[1]))\n",
    "    fpr, tpr, _ = roc_curve(y_bin.ravel(), probs.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, label=f\"Micro-average ROC (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Micro-average ROC (Multi-class)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
